{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c8eddf3-22bb-42de-9aca-53e802d35277",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "\n",
    "1. Extract Sample document and apply following document preprocessing methods: Tokenization,POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document Frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52fa66ac-114a-4cc2-b8ba-6d7533441161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download required NLTK data (run once)\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57f1dc2-ea45-4970-980b-379340ee6102",
   "metadata": {},
   "source": [
    "# Tokenzation of words , sentences :- \n",
    "- we use method word_tokenise() for spliting words from sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c555ca5c-e9b3-4b31-9c69-91a0f66b2376",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'analytics', 'includes', 'preprocessing', 'steps', 'like', 'tokenization', ',', 'stemming', ',', 'and', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Text analytics includes preprocessing steps like tokenization, stemming, and lemmatization.\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fe7ee0a-93c5-4815-87c0-5493244a6545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my name is Abhijit .', 'I am from India']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentence = 'my name is Abhijit . I am from India'\n",
    "print(sent_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b540c4-3caf-41eb-8743-96d506230c40",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fd06076-d637-482e-960d-ed7ff244ff2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Text', 'NN'),\n",
       " ('analytics', 'NNS'),\n",
       " ('includes', 'VBZ'),\n",
       " ('preprocessing', 'VBG'),\n",
       " ('steps', 'NNS'),\n",
       " ('like', 'IN'),\n",
       " ('tokenization', 'NN'),\n",
       " (',', ','),\n",
       " ('stemming', 'VBG'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('lemmatization', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92b701-d892-41d0-895d-7a8ee53f39f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddd24e62-7bb5-4f69-be2f-14ba91ff835e",
   "metadata": {},
   "source": [
    "# Stemming \n",
    "- cutting  down the words that have samw origin but different formats\n",
    "- wait -> waiting , waited , waits\n",
    "- so convert all words to root word wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fc51a94-9aa4-4a9f-b1c1-0ec15e69f346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming for wait is : wait\n",
      "stemming for waiting is : wait\n",
      "stemming for waited is : wait\n",
      "stemming for go is : go\n",
      "stemming for going is : go\n",
      "stemming for here is : here\n",
      "stemming for there is : there\n"
     ]
    }
   ],
   "source": [
    "# simple stemmign example \n",
    "my_words = ['wait','waiting','waited','go','going','here','there']\n",
    "ps = PorterStemmer()\n",
    "for i in my_words:\n",
    "    rootword = ps.stem(i)\n",
    "    print(f\"stemming for {i} is : {rootword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9dc1b6-e30b-4934-84f8-3d3c1b1a732a",
   "metadata": {},
   "source": [
    "# Lemmatization \n",
    "- Why is lemmatization better than stemming\n",
    "  because stemmign does suffix removal but lemmatization does the morphological\n",
    "  analysis of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c3bc590-abe1-4dbf-992f-9ed42a97d6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatization for wait is : wait\n",
      "lemmatization for waiting is : waiting\n",
      "lemmatization for waited is : waited\n",
      "lemmatization for go is : go\n",
      "lemmatization for go is : go\n",
      "lemmatization for here is : here\n",
      "lemmatization for there is : there\n"
     ]
    }
   ],
   "source": [
    "my_words = ['wait','waiting','waited','go','go','here','there']\n",
    "ls = WordNetLemmatizer()\n",
    "for i in my_words:\n",
    "    rootword = ls.lemmatize(i)\n",
    "    print(f\"lemmatization for {i} is : {rootword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae855b3-13d0-489c-8e8c-a3d63c4fc0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a4a7c26-8901-48eb-adc1-2cb53b6fa906",
   "metadata": {},
   "source": [
    "# Stop words \n",
    "- A sentence can have variou stopwords in it like - is , are , the etc\n",
    "- We remove them because they not carry significant meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "325a1f44-683c-46a4-a34c-985121a44097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'Text', 'analytics', 'includes', 'preprocessing', 'steps', 'like', 'tokenization', ',', 'stemming', ',', 'lemmatization', '.']\n"
     ]
    }
   ],
   "source": [
    "stopword = set(stopwords.words('english'))\n",
    "text = \"A Text is analytics includes preprocessing steps like tokenization, stemming, and lemmatization.\"\n",
    "token = word_tokenize(text)\n",
    "filter_words = []\n",
    "\n",
    "for i in token:\n",
    "    if i not in stopword:\n",
    "        filter_words.append(i)\n",
    "\n",
    "print(filter_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b9fa37-cffb-42ae-9c75-4bc4fbbc5a2d",
   "metadata": {},
   "source": [
    "# TF and IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12cb6c4e-3e75-4d4e-8117-54b8a152d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 21 stored elements and shape (3, 20)>\n",
      "  Coords\tValues\n",
      "  (0, 17)\t0.6149219764307087\n",
      "  (0, 0)\t0.30746098821535434\n",
      "  (0, 7)\t0.2338320064840948\n",
      "  (0, 18)\t0.30746098821535434\n",
      "  (0, 13)\t0.30746098821535434\n",
      "  (0, 11)\t0.30746098821535434\n",
      "  (0, 1)\t0.30746098821535434\n",
      "  (0, 4)\t0.30746098821535434\n",
      "  (1, 8)\t0.37796447300922725\n",
      "  (1, 6)\t0.37796447300922725\n",
      "  (1, 16)\t0.37796447300922725\n",
      "  (1, 10)\t0.37796447300922725\n",
      "  (1, 19)\t0.37796447300922725\n",
      "  (1, 2)\t0.37796447300922725\n",
      "  (1, 14)\t0.37796447300922725\n",
      "  (2, 7)\t0.3220024178194947\n",
      "  (2, 9)\t0.4233944834119594\n",
      "  (2, 3)\t0.4233944834119594\n",
      "  (2, 5)\t0.4233944834119594\n",
      "  (2, 12)\t0.4233944834119594\n",
      "  (2, 15)\t0.4233944834119594\n",
      "\n",
      "Feature Names (terms):\n",
      "['analytics' 'analyzing' 'and' 'another' 'data' 'important' 'includes'\n",
      " 'is' 'it' 'lemmatization' 'like' 'of' 'preprocessing' 'process'\n",
      " 'stemming' 'step' 'steps' 'text' 'the' 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"Text analytics is the process of analyzing text data.\",\n",
    "    \"It includes steps like tokenization and stemming.\",\n",
    "    \"Lemmatization is another important preprocessing step.\"\n",
    "]\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform\n",
    "tfidf_matrix = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Show results\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix)\n",
    "\n",
    "print(\"\\nFeature Names (terms):\")\n",
    "print(vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d9f43-56a5-4949-8e9b-533c7c6cb25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc43b7d3-4206-4640-9370-88f5144845f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde1ad9-1eb0-4b8d-ac5b-5c90dce2ec75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f200bf0-c043-4906-af68-d4a9e38f09a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d40f6e-1f4b-4be9-9800-57fc48560727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d47c34-720b-4ec2-8d2f-67caea7c3f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915237ca-c037-4769-9b66-a0350cf51672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5120b33a-190b-48c0-bc00-71b49656c767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b4a441-117f-4ceb-a34f-0176502f8a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
